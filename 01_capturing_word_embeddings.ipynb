{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-capturing-word-embeddings.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/iricelino/hybridNLP2018/blob/master/01_capturing_word_embeddings.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "LPx_bqtS7OCj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download a small text corpus\n",
        "First, let's download a corpus into our environment. We will use a small sample of the UMBC corpus that has been pre-tokenized and that we have included as part of our GitHub repository. First, we will clone the repo so we have access to it from this environment."
      ]
    },
    {
      "metadata": {
        "id": "Zc-8xkaofomg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UjX5TGjMeLT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "56d1153b-0a56-49f7-cea1-c8ba7c8b35a4"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hybridNLP2018/tutorial.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tutorial'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 601 (delta 1), reused 6 (delta 1), pack-reused 592\u001b[K\n",
            "Receiving objects: 100% (601/601), 58.56 MiB | 40.38 MiB/s, done.\n",
            "Resolving deltas: 100% (338/338), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aTTPhI_liU-u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generate word embeddings using Swivel\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this notebook we show how to generate word embeddings based on the K-Cap corpus using the [Swivel algorithm](https://arxiv.org/pdf/1602.02215). In particular, we reuse the implementation included in the [Tensorflow models repo on Github](https://github.com/tensorflow/models/tree/master/research/swivel) (with some small modifications)."
      ]
    },
    {
      "metadata": {
        "id": "lSNfGrTQ7qiN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The dataset comes as a zip file, so we unzip it by executing the following cell. We also define a variable pointing to the corpus file:"
      ]
    },
    {
      "metadata": {
        "id": "CUFY-Sl8lFDK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "859b4341-b212-45ba-fb40-d1809d8019b8"
      },
      "cell_type": "code",
      "source": [
        "!unzip /content/tutorial/datasamples/umbc_t_5K.zip -d /content/tutorial/datasamples/\n",
        "input_corpus='/content/tutorial/datasamples/umbc_t_5K'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/tutorial/datasamples/umbc_t_5K.zip\n",
            "  inflating: /content/tutorial/datasamples/umbc_t_5K  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0ymuxrBHQr7u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can inspect the file using the `%less` command to print the whole input file at the bottom of the screen. It'll be quicker to just print a few lines:"
      ]
    },
    {
      "metadata": {
        "id": "43SExjL46jdZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "abbeab31-5576-4737-974b-e2f9fab5f4fd"
      },
      "cell_type": "code",
      "source": [
        "#%less {input_corpus}\n",
        "!head -n1 {input_corpus}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the mayan image collection was contributed by oberlin college faculty and library staff . professor linda grimm , associate professor of anthropology and project coordinator at oberlin , explained the educational goals for this online project .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ju7vkSHcNmcS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The output above shows that the input text has already been pre-processed. \n",
        " * All words have been converted to lower-case (this will avoid having two separate words for *The* and *the*)\n",
        " * punctuation marks have been separated from words. This will avoid creating \"words\" such as \"staff.\" or \"grimm,\" in the example above."
      ]
    },
    {
      "metadata": {
        "id": "1PyH6EI783Kw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## `swivel`: an algorithm for learning word embeddings\n",
        "Now that we have a corpus, we need an (implementation of an) algorithm for learning embeddings. There are various libraries and implementations for this:\n",
        "  * [word2vec](https://pypi.org/project/word2vec/) the system proposed by Mikolov that introduced many of the techniques now commonly used for learning word embeddings. It directly generates word embeddings from the text corpus by using a sliding window and trying to predict a target word based on neighbouring context words.\n",
        "  * [GloVe](https://github.com/stanfordnlp/GloVe) an alternative algorithm by Pennington, Socher and Manning. It splits the process in two steps: \n",
        "    1. calculating a word-word co-occurrence matrix \n",
        "    2. learning embeddings from this matrix\n",
        "  * [FastText](https://fasttext.cc/) is a more recent algorithm by Mikolov et al (now at Facebook) that extends the original word2vec algorithm in various ways. Among others, this algorithm takes into accout subword information.\n",
        "  \n",
        "In this tutorial we will be using [Swivel](https://github.com/tensorflow/models/tree/master/research/swivel) an algorithm similar to GloVe, which makes it easier to extend to include both words and concepts (which we will do in [notebook 03 vecsigrafo](https://colab.research.google.com/github/HybridNLP2018/tutorial/blob/master/03_vecsigrafo.ipynb)). As with GloVe, Swivel first extracts a word-word co-occurence matrix from a text corpus and then uses this matrix to learn the embeddings.\n",
        "\n",
        "The official  [Swivel](https://github.com/tensorflow/models/tree/master/research/swivel)  implementation has a few issues when running on Colaboratory, hence we have included a slightly modified version as part of the HybridNLP2018 github repository. "
      ]
    },
    {
      "metadata": {
        "id": "8A0IVDoDTU3_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "a6fba3bd-bf7c-4cb3-c818-13d11fee5727"
      },
      "cell_type": "code",
      "source": [
        "%ls /content/tutorial/scripts/swivel/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "analogy.cc      fastprep.cc         __init__.py  README.md    vecs.py\n",
            "distributed.sh  fastprep.mk         nearest.py   swivel.py    wordsim.py\n",
            "eval.mk         glove_to_shards.py  prep.py      text2bin.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7xppOkdyiU-1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Learn embeddings\n",
        "\n",
        "### Generate co-occurrence matrix using Swivel `prep`\n",
        "\n",
        "Call swivel's `prep` command to calculate the word co-occurrence matrix. We use the `%run` magic command, which runs the named python file as a program, allowing us to pass parameters as if using a command-line terminal.\n",
        "\n",
        "We set the `shard_size` to 512 since the corpus is quite small. For larger corpora we could use the standard value of 4096."
      ]
    },
    {
      "metadata": {
        "id": "8wi_izVOiU-2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        },
        "outputId": "f4a39ef2-aca3-4a47-bde2-93713ab736e2"
      },
      "cell_type": "code",
      "source": [
        "coocs_path = '/content/umbc/coocs/t_5K/'\n",
        "shard_size = 512\n",
        "!python /content/tutorial/scripts/swivel/prep.py \\\n",
        "  --input=\"/content/tutorial/datasamples/umbc_t_5K\" \\\n",
        "  --output_dir=\"/content/umbc/coocs/t_5K/\" \\\n",
        "  --shard_size=512"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running with flags \n",
            "/content/tutorial/scripts/swivel/prep.py:\n",
            "  --bufsz: The number of co-occurrences to buffer\n",
            "    (default: '16777216')\n",
            "    (an integer)\n",
            "  --input: The input text.\n",
            "    (default: '')\n",
            "  --max_vocab: The maximum vocabulary size\n",
            "    (default: '1048576')\n",
            "    (an integer)\n",
            "  --min_count: The minimum number of times a word should occur to be included in\n",
            "    the vocabulary\n",
            "    (default: '5')\n",
            "    (an integer)\n",
            "  --output_dir: Output directory for Swivel data\n",
            "    (default: '/tmp/swivel_data')\n",
            "  --shard_size: The size for each shard\n",
            "    (default: '4096')\n",
            "    (an integer)\n",
            "  --vocab: Vocabulary to use instead of generating one\n",
            "    (default: '')\n",
            "  --window_size: The window size\n",
            "    (default: '10')\n",
            "    (an integer)\n",
            "\n",
            "tensorflow.python.platform.app:\n",
            "  -h,--[no]help: show this help\n",
            "    (default: 'false')\n",
            "  --[no]helpfull: show full help\n",
            "    (default: 'false')\n",
            "  --[no]helpshort: show this help\n",
            "    (default: 'false')\n",
            "\n",
            "absl.flags:\n",
            "  --flagfile: Insert flag definitions from the given file into the command line.\n",
            "    (default: '')\n",
            "  --undefok: comma-separated list of flag names that it is okay to specify on\n",
            "    the command line even if the program does not define a flag with that name.\n",
            "    IMPORTANT: flags in this list that have arguments MUST use the --flag=value\n",
            "    format.\n",
            "    (default: '')\n",
            "\n",
            "vocabulary contains 5120 tokens\n",
            "\n",
            "writing shard 100/100\n",
            "Wrote vocab and sum files to /content/umbc/coocs/t_5K/\n",
            "Wrote vocab and sum files to /content/umbc/coocs/t_5K/\n",
            "done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5fyTfMneQnQc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The expected output is:\n",
        "\n",
        "```\n",
        "   ... tensorflow parameters ...\n",
        "    vocabulary contains 5120 tokens\n",
        "\n",
        "    writing shard 100/100\n",
        "    done!\n",
        "```\n",
        "\n",
        "We see that first, the algorithm determined the **vocabulary** $V$, this is the list of words for which an embedding will be generated. Since the corpus is fairly small, so is the vocabulary, which consists of only about 5K words (large corpora can result in vocabularies with millions of words).\n",
        "\n",
        "The co-occurrence matrix is a sparse matrix of $|V| \\times |V|$ elements. Swivel uses shards to create submatrices of $|S| \\times |S|$, where $S$ is the shard-size specified above. In this case, we have 100 sub-matrices.\n",
        "\n",
        "All this information is stored in the output folder we specified above. It consists of  100 files, one per shard/sub-matrix and a few additional files:"
      ]
    },
    {
      "metadata": {
        "id": "3eiJEg8mSOlJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0edfacb7-8160-4820-bc5b-f618f0087b2d"
      },
      "cell_type": "code",
      "source": [
        "%ls {coocs_path} | head -n 10"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "col_sums.txt\n",
            "col_vocab.txt\n",
            "row_sums.txt\n",
            "row_vocab.txt\n",
            "shard-000-000.pb\n",
            "shard-000-001.pb\n",
            "shard-000-002.pb\n",
            "shard-000-003.pb\n",
            "shard-000-004.pb\n",
            "shard-000-005.pb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AMm9rXSbiU-8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "The `prep` step does the following:\n",
        "  - it uses a basic, white space, tokenization to get sequences of tokens\n",
        "  - in a first pass through the corpus, it counts all tokens and keeps only those that have a minimum frequency (5) in the corpus. Then it keeps a multiple of the `shard_size` of that. The tokens that are kept form the **vocabulary** with size $v = |V|$.\n",
        "  - on a second pass through the corpus, it uses a sliding window to count co-occurrences between the focus token and the context tokens (similar to `word2vec`). The result is a sparse co-occurrence matrix of size $v \\times v$.\n",
        "  - for easier storage and manipulation, Swivel uses *sharding* to split the co-occurrence matrix into sub-matrices of size $s \\times s$, where $s$ is the `shard_size`.\n",
        "  ![Swivel co-occurrence matrix sharding](https://github.com/hybridNLP2018/tutorial/blob/master/images/swivel-sharding.PNG?raw=1)\n",
        "  - store the sharded co-occurrence submatrices as [protobuf files](https://developers.google.com/protocol-buffers/)."
      ]
    },
    {
      "metadata": {
        "id": "o9tIXHWbiU-9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Learn embeddings from co-occurrence matrix\n",
        "With the sharded co-occurrence matrix it is now possible to learn embeddings:\n",
        " - the input is the folder with the co-occurrence matrix (protobuf files with the sparse matrix).\n",
        " - `submatrix_` rows and columns need to be the same size as the `shard_size` used in the `prep` step."
      ]
    },
    {
      "metadata": {
        "id": "nyk4vjv6iU--",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8022
        },
        "outputId": "ad37320d-fda5-4e06-e4b1-2d5c1c4f2ff8"
      },
      "cell_type": "code",
      "source": [
        "vec_path = '/content/umbc/vec/t_5K/'\n",
        "!python /content/tutorial/scripts/swivel/swivel.py --input_base_path={coocs_path} \\\n",
        "    --output_base_path={vec_path} \\\n",
        "    --num_epochs=40 --dim=150 \\\n",
        "    --submatrix_rows={shard_size} --submatrix_cols={shard_size}"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:187: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /content/tutorial/scripts/swivel/swivel.py:495: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "INFO:tensorflow:Saving checkpoint to path /content/umbc/vec/t_5K/model.ckpt\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "INFO:tensorflow:global_step/sec: 0\n",
            "INFO:tensorflow:Recording summary at step 0.\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n",
            "INFO:tensorflow:local_step=10 global_step=10 loss=56.9, 0.2% complete\n",
            "INFO:tensorflow:local_step=20 global_step=20 loss=56.3, 0.5% complete\n",
            "INFO:tensorflow:local_step=30 global_step=30 loss=53.6, 0.8% complete\n",
            "INFO:tensorflow:local_step=40 global_step=40 loss=55.6, 1.0% complete\n",
            "INFO:tensorflow:local_step=50 global_step=50 loss=43.9, 1.2% complete\n",
            "INFO:tensorflow:local_step=60 global_step=60 loss=51.1, 1.5% complete\n",
            "INFO:tensorflow:local_step=70 global_step=70 loss=37.8, 1.8% complete\n",
            "INFO:tensorflow:local_step=80 global_step=80 loss=49.0, 2.0% complete\n",
            "INFO:tensorflow:local_step=90 global_step=90 loss=73.2, 2.2% complete\n",
            "INFO:tensorflow:local_step=100 global_step=100 loss=28.0, 2.5% complete\n",
            "INFO:tensorflow:local_step=110 global_step=110 loss=43.7, 2.8% complete\n",
            "INFO:tensorflow:local_step=120 global_step=120 loss=30.7, 3.0% complete\n",
            "INFO:tensorflow:local_step=130 global_step=130 loss=37.1, 3.2% complete\n",
            "INFO:tensorflow:local_step=140 global_step=140 loss=83.9, 3.5% complete\n",
            "INFO:tensorflow:local_step=150 global_step=150 loss=94.2, 3.8% complete\n",
            "INFO:tensorflow:local_step=160 global_step=160 loss=35.1, 4.0% complete\n",
            "INFO:tensorflow:local_step=170 global_step=170 loss=32.2, 4.2% complete\n",
            "INFO:tensorflow:local_step=180 global_step=180 loss=32.7, 4.5% complete\n",
            "INFO:tensorflow:local_step=190 global_step=190 loss=41.3, 4.8% complete\n",
            "INFO:tensorflow:local_step=200 global_step=200 loss=26.4, 5.0% complete\n",
            "INFO:tensorflow:local_step=210 global_step=210 loss=38.3, 5.2% complete\n",
            "INFO:tensorflow:local_step=220 global_step=220 loss=28.3, 5.5% complete\n",
            "INFO:tensorflow:local_step=230 global_step=230 loss=33.9, 5.8% complete\n",
            "INFO:tensorflow:local_step=240 global_step=240 loss=111.8, 6.0% complete\n",
            "INFO:tensorflow:local_step=250 global_step=250 loss=27.8, 6.2% complete\n",
            "INFO:tensorflow:local_step=260 global_step=260 loss=34.6, 6.5% complete\n",
            "INFO:tensorflow:local_step=270 global_step=270 loss=32.1, 6.8% complete\n",
            "INFO:tensorflow:local_step=280 global_step=280 loss=132.0, 7.0% complete\n",
            "INFO:tensorflow:local_step=290 global_step=290 loss=32.8, 7.2% complete\n",
            "INFO:tensorflow:local_step=300 global_step=300 loss=24.9, 7.5% complete\n",
            "INFO:tensorflow:local_step=310 global_step=310 loss=30.3, 7.8% complete\n",
            "INFO:tensorflow:local_step=320 global_step=320 loss=33.2, 8.0% complete\n",
            "INFO:tensorflow:local_step=330 global_step=330 loss=119.6, 8.2% complete\n",
            "INFO:tensorflow:local_step=340 global_step=340 loss=28.8, 8.5% complete\n",
            "INFO:tensorflow:local_step=350 global_step=350 loss=32.8, 8.8% complete\n",
            "INFO:tensorflow:local_step=360 global_step=360 loss=30.1, 9.0% complete\n",
            "INFO:tensorflow:local_step=370 global_step=370 loss=28.7, 9.2% complete\n",
            "INFO:tensorflow:local_step=380 global_step=380 loss=157.5, 9.5% complete\n",
            "INFO:tensorflow:local_step=390 global_step=390 loss=29.2, 9.8% complete\n",
            "INFO:tensorflow:local_step=400 global_step=400 loss=31.7, 10.0% complete\n",
            "INFO:tensorflow:local_step=410 global_step=410 loss=22.3, 10.2% complete\n",
            "INFO:tensorflow:local_step=420 global_step=420 loss=32.5, 10.5% complete\n",
            "INFO:tensorflow:local_step=430 global_step=430 loss=32.4, 10.8% complete\n",
            "INFO:tensorflow:local_step=440 global_step=440 loss=182.9, 11.0% complete\n",
            "INFO:tensorflow:local_step=450 global_step=450 loss=31.6, 11.2% complete\n",
            "INFO:tensorflow:local_step=460 global_step=460 loss=34.6, 11.5% complete\n",
            "INFO:tensorflow:local_step=470 global_step=470 loss=30.8, 11.8% complete\n",
            "INFO:tensorflow:local_step=480 global_step=480 loss=35.6, 12.0% complete\n",
            "INFO:tensorflow:local_step=490 global_step=490 loss=28.9, 12.2% complete\n",
            "INFO:tensorflow:local_step=500 global_step=500 loss=22.8, 12.5% complete\n",
            "INFO:tensorflow:local_step=510 global_step=510 loss=28.8, 12.8% complete\n",
            "INFO:tensorflow:local_step=520 global_step=520 loss=29.6, 13.0% complete\n",
            "INFO:tensorflow:local_step=530 global_step=530 loss=28.9, 13.2% complete\n",
            "INFO:tensorflow:local_step=540 global_step=540 loss=30.5, 13.5% complete\n",
            "INFO:tensorflow:local_step=550 global_step=550 loss=138.1, 13.8% complete\n",
            "INFO:tensorflow:local_step=560 global_step=560 loss=28.1, 14.0% complete\n",
            "INFO:tensorflow:local_step=570 global_step=570 loss=26.6, 14.2% complete\n",
            "INFO:tensorflow:local_step=580 global_step=580 loss=27.1, 14.5% complete\n",
            "INFO:tensorflow:local_step=590 global_step=590 loss=28.2, 14.8% complete\n",
            "INFO:tensorflow:local_step=600 global_step=600 loss=30.7, 15.0% complete\n",
            "INFO:tensorflow:local_step=610 global_step=610 loss=30.3, 15.2% complete\n",
            "INFO:tensorflow:local_step=620 global_step=620 loss=29.0, 15.5% complete\n",
            "INFO:tensorflow:local_step=630 global_step=630 loss=77.3, 15.8% complete\n",
            "INFO:tensorflow:local_step=640 global_step=640 loss=34.6, 16.0% complete\n",
            "INFO:tensorflow:local_step=650 global_step=650 loss=34.4, 16.2% complete\n",
            "INFO:tensorflow:local_step=660 global_step=660 loss=33.6, 16.5% complete\n",
            "INFO:tensorflow:local_step=670 global_step=670 loss=32.1, 16.8% complete\n",
            "INFO:tensorflow:local_step=680 global_step=680 loss=31.3, 17.0% complete\n",
            "INFO:tensorflow:local_step=690 global_step=690 loss=34.1, 17.2% complete\n",
            "INFO:tensorflow:local_step=700 global_step=700 loss=25.8, 17.5% complete\n",
            "INFO:tensorflow:local_step=710 global_step=710 loss=24.9, 17.8% complete\n",
            "INFO:tensorflow:local_step=720 global_step=720 loss=32.4, 18.0% complete\n",
            "INFO:tensorflow:local_step=730 global_step=730 loss=28.9, 18.2% complete\n",
            "INFO:tensorflow:local_step=740 global_step=740 loss=29.2, 18.5% complete\n",
            "INFO:tensorflow:local_step=750 global_step=750 loss=124.9, 18.8% complete\n",
            "INFO:tensorflow:local_step=760 global_step=760 loss=34.9, 19.0% complete\n",
            "INFO:tensorflow:local_step=770 global_step=770 loss=30.3, 19.2% complete\n",
            "INFO:tensorflow:local_step=780 global_step=780 loss=33.5, 19.5% complete\n",
            "INFO:tensorflow:local_step=790 global_step=790 loss=30.6, 19.8% complete\n",
            "INFO:tensorflow:local_step=800 global_step=800 loss=25.2, 20.0% complete\n",
            "INFO:tensorflow:local_step=810 global_step=810 loss=29.4, 20.2% complete\n",
            "INFO:tensorflow:local_step=820 global_step=820 loss=27.8, 20.5% complete\n",
            "INFO:tensorflow:local_step=830 global_step=830 loss=28.8, 20.8% complete\n",
            "INFO:tensorflow:local_step=840 global_step=840 loss=28.0, 21.0% complete\n",
            "INFO:tensorflow:local_step=850 global_step=850 loss=30.6, 21.2% complete\n",
            "INFO:tensorflow:local_step=860 global_step=860 loss=30.7, 21.5% complete\n",
            "INFO:tensorflow:local_step=870 global_step=870 loss=34.1, 21.8% complete\n",
            "INFO:tensorflow:local_step=880 global_step=880 loss=35.8, 22.0% complete\n",
            "INFO:tensorflow:local_step=890 global_step=890 loss=30.7, 22.2% complete\n",
            "INFO:tensorflow:local_step=900 global_step=900 loss=31.7, 22.5% complete\n",
            "INFO:tensorflow:local_step=910 global_step=910 loss=26.5, 22.8% complete\n",
            "INFO:tensorflow:local_step=920 global_step=920 loss=29.5, 23.0% complete\n",
            "INFO:tensorflow:local_step=930 global_step=930 loss=23.2, 23.2% complete\n",
            "INFO:tensorflow:local_step=940 global_step=940 loss=29.4, 23.5% complete\n",
            "INFO:tensorflow:local_step=950 global_step=950 loss=32.8, 23.8% complete\n",
            "INFO:tensorflow:local_step=960 global_step=960 loss=32.3, 24.0% complete\n",
            "INFO:tensorflow:local_step=970 global_step=970 loss=31.6, 24.2% complete\n",
            "INFO:tensorflow:local_step=980 global_step=980 loss=30.6, 24.5% complete\n",
            "INFO:tensorflow:local_step=990 global_step=990 loss=29.3, 24.8% complete\n",
            "INFO:tensorflow:local_step=1000 global_step=1000 loss=88.2, 25.0% complete\n",
            "INFO:tensorflow:local_step=1010 global_step=1010 loss=29.2, 25.2% complete\n",
            "INFO:tensorflow:local_step=1020 global_step=1020 loss=28.7, 25.5% complete\n",
            "INFO:tensorflow:local_step=1030 global_step=1030 loss=29.0, 25.8% complete\n",
            "INFO:tensorflow:local_step=1040 global_step=1040 loss=31.1, 26.0% complete\n",
            "INFO:tensorflow:local_step=1050 global_step=1050 loss=27.1, 26.2% complete\n",
            "INFO:tensorflow:local_step=1060 global_step=1060 loss=35.1, 26.5% complete\n",
            "INFO:tensorflow:local_step=1070 global_step=1070 loss=29.5, 26.8% complete\n",
            "INFO:tensorflow:local_step=1080 global_step=1080 loss=27.4, 27.0% complete\n",
            "INFO:tensorflow:local_step=1090 global_step=1090 loss=29.1, 27.2% complete\n",
            "INFO:tensorflow:local_step=1100 global_step=1100 loss=28.3, 27.5% complete\n",
            "INFO:tensorflow:local_step=1110 global_step=1110 loss=96.8, 27.8% complete\n",
            "INFO:tensorflow:local_step=1120 global_step=1120 loss=32.2, 28.0% complete\n",
            "INFO:tensorflow:local_step=1130 global_step=1130 loss=25.9, 28.2% complete\n",
            "INFO:tensorflow:local_step=1140 global_step=1140 loss=31.6, 28.5% complete\n",
            "INFO:tensorflow:local_step=1150 global_step=1150 loss=33.2, 28.8% complete\n",
            "INFO:tensorflow:local_step=1160 global_step=1160 loss=28.9, 29.0% complete\n",
            "INFO:tensorflow:local_step=1170 global_step=1170 loss=31.0, 29.2% complete\n",
            "INFO:tensorflow:local_step=1180 global_step=1180 loss=32.3, 29.5% complete\n",
            "INFO:tensorflow:local_step=1190 global_step=1190 loss=25.3, 29.8% complete\n",
            "INFO:tensorflow:local_step=1200 global_step=1200 loss=109.5, 30.0% complete\n",
            "INFO:tensorflow:local_step=1210 global_step=1210 loss=27.0, 30.2% complete\n",
            "INFO:tensorflow:local_step=1220 global_step=1220 loss=27.9, 30.5% complete\n",
            "INFO:tensorflow:local_step=1230 global_step=1230 loss=23.4, 30.8% complete\n",
            "INFO:tensorflow:local_step=1240 global_step=1240 loss=33.2, 31.0% complete\n",
            "INFO:tensorflow:local_step=1250 global_step=1250 loss=27.5, 31.2% complete\n",
            "INFO:tensorflow:local_step=1260 global_step=1260 loss=26.5, 31.5% complete\n",
            "INFO:tensorflow:local_step=1270 global_step=1270 loss=32.3, 31.8% complete\n",
            "INFO:tensorflow:local_step=1280 global_step=1280 loss=28.0, 32.0% complete\n",
            "INFO:tensorflow:local_step=1290 global_step=1290 loss=194.8, 32.2% complete\n",
            "INFO:tensorflow:local_step=1300 global_step=1300 loss=26.8, 32.5% complete\n",
            "INFO:tensorflow:local_step=1310 global_step=1310 loss=28.6, 32.8% complete\n",
            "INFO:tensorflow:local_step=1320 global_step=1320 loss=29.7, 33.0% complete\n",
            "INFO:tensorflow:local_step=1330 global_step=1330 loss=30.1, 33.2% complete\n",
            "INFO:tensorflow:local_step=1340 global_step=1340 loss=34.5, 33.5% complete\n",
            "INFO:tensorflow:local_step=1350 global_step=1350 loss=32.8, 33.8% complete\n",
            "INFO:tensorflow:local_step=1360 global_step=1360 loss=31.8, 34.0% complete\n",
            "INFO:tensorflow:local_step=1370 global_step=1370 loss=28.0, 34.2% complete\n",
            "INFO:tensorflow:local_step=1380 global_step=1380 loss=28.4, 34.5% complete\n",
            "INFO:tensorflow:local_step=1390 global_step=1390 loss=30.2, 34.8% complete\n",
            "INFO:tensorflow:local_step=1400 global_step=1400 loss=24.6, 35.0% complete\n",
            "INFO:tensorflow:local_step=1410 global_step=1410 loss=27.4, 35.2% complete\n",
            "INFO:tensorflow:local_step=1420 global_step=1420 loss=114.3, 35.5% complete\n",
            "INFO:tensorflow:local_step=1430 global_step=1430 loss=27.8, 35.8% complete\n",
            "INFO:tensorflow:local_step=1440 global_step=1440 loss=31.4, 36.0% complete\n",
            "INFO:tensorflow:local_step=1450 global_step=1450 loss=26.8, 36.2% complete\n",
            "INFO:tensorflow:local_step=1460 global_step=1460 loss=148.0, 36.5% complete\n",
            "INFO:tensorflow:local_step=1470 global_step=1470 loss=30.2, 36.8% complete\n",
            "INFO:tensorflow:local_step=1480 global_step=1480 loss=30.5, 37.0% complete\n",
            "INFO:tensorflow:local_step=1490 global_step=1490 loss=31.0, 37.2% complete\n",
            "INFO:tensorflow:local_step=1500 global_step=1500 loss=26.1, 37.5% complete\n",
            "INFO:tensorflow:local_step=1510 global_step=1510 loss=27.5, 37.8% complete\n",
            "INFO:tensorflow:local_step=1520 global_step=1520 loss=30.3, 38.0% complete\n",
            "INFO:tensorflow:local_step=1530 global_step=1530 loss=29.7, 38.2% complete\n",
            "INFO:tensorflow:local_step=1540 global_step=1540 loss=27.2, 38.5% complete\n",
            "INFO:tensorflow:local_step=1550 global_step=1550 loss=28.2, 38.8% complete\n",
            "INFO:tensorflow:local_step=1560 global_step=1560 loss=30.3, 39.0% complete\n",
            "INFO:tensorflow:local_step=1570 global_step=1570 loss=30.3, 39.2% complete\n",
            "INFO:tensorflow:local_step=1580 global_step=1580 loss=27.7, 39.5% complete\n",
            "INFO:tensorflow:local_step=1590 global_step=1590 loss=30.8, 39.8% complete\n",
            "INFO:tensorflow:local_step=1600 global_step=1600 loss=24.9, 40.0% complete\n",
            "INFO:tensorflow:local_step=1610 global_step=1610 loss=29.8, 40.2% complete\n",
            "INFO:tensorflow:local_step=1620 global_step=1620 loss=25.6, 40.5% complete\n",
            "INFO:tensorflow:local_step=1630 global_step=1630 loss=30.9, 40.8% complete\n",
            "INFO:tensorflow:local_step=1640 global_step=1640 loss=28.6, 41.0% complete\n",
            "INFO:tensorflow:local_step=1650 global_step=1650 loss=29.9, 41.2% complete\n",
            "INFO:tensorflow:local_step=1660 global_step=1660 loss=30.8, 41.5% complete\n",
            "INFO:tensorflow:local_step=1670 global_step=1670 loss=178.4, 41.8% complete\n",
            "INFO:tensorflow:local_step=1680 global_step=1680 loss=31.2, 42.0% complete\n",
            "INFO:tensorflow:local_step=1690 global_step=1690 loss=30.6, 42.2% complete\n",
            "INFO:tensorflow:local_step=1700 global_step=1700 loss=27.4, 42.5% complete\n",
            "INFO:tensorflow:local_step=1710 global_step=1710 loss=25.5, 42.8% complete\n",
            "INFO:tensorflow:local_step=1720 global_step=1720 loss=28.7, 43.0% complete\n",
            "INFO:tensorflow:local_step=1730 global_step=1730 loss=31.5, 43.2% complete\n",
            "INFO:tensorflow:local_step=1740 global_step=1740 loss=26.3, 43.5% complete\n",
            "INFO:tensorflow:local_step=1750 global_step=1750 loss=135.9, 43.8% complete\n",
            "INFO:tensorflow:local_step=1760 global_step=1760 loss=29.0, 44.0% complete\n",
            "INFO:tensorflow:local_step=1770 global_step=1770 loss=28.8, 44.2% complete\n",
            "INFO:tensorflow:local_step=1780 global_step=1780 loss=178.4, 44.5% complete\n",
            "INFO:tensorflow:local_step=1790 global_step=1790 loss=29.5, 44.8% complete\n",
            "INFO:tensorflow:local_step=1800 global_step=1800 loss=27.4, 45.0% complete\n",
            "INFO:tensorflow:local_step=1810 global_step=1810 loss=25.3, 45.2% complete\n",
            "INFO:tensorflow:local_step=1820 global_step=1820 loss=32.7, 45.5% complete\n",
            "INFO:tensorflow:local_step=1830 global_step=1830 loss=26.6, 45.8% complete\n",
            "INFO:tensorflow:local_step=1840 global_step=1840 loss=29.9, 46.0% complete\n",
            "INFO:tensorflow:local_step=1850 global_step=1850 loss=32.0, 46.2% complete\n",
            "INFO:tensorflow:local_step=1860 global_step=1860 loss=33.7, 46.5% complete\n",
            "INFO:tensorflow:local_step=1870 global_step=1870 loss=33.1, 46.8% complete\n",
            "INFO:tensorflow:local_step=1880 global_step=1880 loss=30.2, 47.0% complete\n",
            "INFO:tensorflow:local_step=1890 global_step=1890 loss=30.5, 47.2% complete\n",
            "INFO:tensorflow:local_step=1900 global_step=1900 loss=28.0, 47.5% complete\n",
            "INFO:tensorflow:local_step=1910 global_step=1910 loss=26.5, 47.8% complete\n",
            "INFO:tensorflow:local_step=1920 global_step=1920 loss=29.1, 48.0% complete\n",
            "INFO:tensorflow:local_step=1930 global_step=1930 loss=31.6, 48.2% complete\n",
            "INFO:tensorflow:local_step=1940 global_step=1940 loss=23.5, 48.5% complete\n",
            "INFO:tensorflow:local_step=1950 global_step=1950 loss=31.7, 48.8% complete\n",
            "INFO:tensorflow:local_step=1960 global_step=1960 loss=29.2, 49.0% complete\n",
            "INFO:tensorflow:local_step=1970 global_step=1970 loss=155.6, 49.2% complete\n",
            "INFO:tensorflow:local_step=1980 global_step=1980 loss=28.1, 49.5% complete\n",
            "INFO:tensorflow:local_step=1990 global_step=1990 loss=27.3, 49.8% complete\n",
            "INFO:tensorflow:local_step=2000 global_step=2000 loss=21.6, 50.0% complete\n",
            "INFO:tensorflow:local_step=2010 global_step=2010 loss=39.7, 50.2% complete\n",
            "INFO:tensorflow:local_step=2020 global_step=2020 loss=30.5, 50.5% complete\n",
            "INFO:tensorflow:local_step=2030 global_step=2030 loss=29.2, 50.8% complete\n",
            "INFO:tensorflow:local_step=2040 global_step=2040 loss=26.5, 51.0% complete\n",
            "INFO:tensorflow:local_step=2050 global_step=2050 loss=28.2, 51.2% complete\n",
            "INFO:tensorflow:local_step=2060 global_step=2060 loss=34.3, 51.5% complete\n",
            "INFO:tensorflow:local_step=2070 global_step=2070 loss=33.4, 51.8% complete\n",
            "INFO:tensorflow:local_step=2080 global_step=2080 loss=29.8, 52.0% complete\n",
            "INFO:tensorflow:local_step=2090 global_step=2090 loss=28.8, 52.2% complete\n",
            "INFO:tensorflow:local_step=2100 global_step=2100 loss=22.4, 52.5% complete\n",
            "INFO:tensorflow:local_step=2110 global_step=2110 loss=28.2, 52.8% complete\n",
            "INFO:tensorflow:local_step=2120 global_step=2120 loss=32.5, 53.0% complete\n",
            "INFO:tensorflow:local_step=2130 global_step=2130 loss=32.1, 53.2% complete\n",
            "INFO:tensorflow:local_step=2140 global_step=2140 loss=25.6, 53.5% complete\n",
            "INFO:tensorflow:local_step=2150 global_step=2150 loss=31.5, 53.8% complete\n",
            "INFO:tensorflow:local_step=2160 global_step=2160 loss=29.8, 54.0% complete\n",
            "INFO:tensorflow:local_step=2170 global_step=2170 loss=28.5, 54.2% complete\n",
            "INFO:tensorflow:local_step=2180 global_step=2180 loss=28.4, 54.5% complete\n",
            "INFO:tensorflow:local_step=2190 global_step=2190 loss=172.5, 54.8% complete\n",
            "INFO:tensorflow:local_step=2200 global_step=2200 loss=30.0, 55.0% complete\n",
            "INFO:tensorflow:local_step=2210 global_step=2210 loss=31.7, 55.2% complete\n",
            "INFO:tensorflow:local_step=2220 global_step=2220 loss=135.5, 55.5% complete\n",
            "INFO:tensorflow:local_step=2230 global_step=2230 loss=30.2, 55.8% complete\n",
            "INFO:tensorflow:local_step=2240 global_step=2240 loss=30.1, 56.0% complete\n",
            "INFO:tensorflow:local_step=2250 global_step=2250 loss=31.8, 56.2% complete\n",
            "INFO:tensorflow:local_step=2260 global_step=2260 loss=28.9, 56.5% complete\n",
            "INFO:tensorflow:local_step=2270 global_step=2270 loss=24.5, 56.8% complete\n",
            "INFO:tensorflow:local_step=2280 global_step=2280 loss=29.8, 57.0% complete\n",
            "INFO:tensorflow:local_step=2290 global_step=2290 loss=30.0, 57.2% complete\n",
            "INFO:tensorflow:local_step=2300 global_step=2300 loss=23.4, 57.5% complete\n",
            "INFO:tensorflow:local_step=2310 global_step=2310 loss=26.8, 57.8% complete\n",
            "INFO:tensorflow:local_step=2320 global_step=2320 loss=123.8, 58.0% complete\n",
            "INFO:tensorflow:local_step=2330 global_step=2330 loss=25.2, 58.2% complete\n",
            "INFO:tensorflow:local_step=2340 global_step=2340 loss=29.6, 58.5% complete\n",
            "INFO:tensorflow:local_step=2350 global_step=2350 loss=134.9, 58.8% complete\n",
            "INFO:tensorflow:local_step=2360 global_step=2360 loss=27.8, 59.0% complete\n",
            "INFO:tensorflow:local_step=2370 global_step=2370 loss=212.0, 59.2% complete\n",
            "INFO:tensorflow:local_step=2380 global_step=2380 loss=30.4, 59.5% complete\n",
            "INFO:tensorflow:local_step=2390 global_step=2390 loss=29.1, 59.8% complete\n",
            "INFO:tensorflow:local_step=2400 global_step=2400 loss=25.9, 60.0% complete\n",
            "INFO:tensorflow:local_step=2410 global_step=2410 loss=24.9, 60.2% complete\n",
            "INFO:tensorflow:local_step=2420 global_step=2420 loss=22.2, 60.5% complete\n",
            "INFO:tensorflow:local_step=2430 global_step=2430 loss=28.9, 60.8% complete\n",
            "INFO:tensorflow:local_step=2440 global_step=2440 loss=27.9, 61.0% complete\n",
            "INFO:tensorflow:local_step=2450 global_step=2450 loss=29.6, 61.2% complete\n",
            "INFO:tensorflow:local_step=2460 global_step=2460 loss=31.6, 61.5% complete\n",
            "INFO:tensorflow:local_step=2470 global_step=2470 loss=157.6, 61.8% complete\n",
            "INFO:tensorflow:local_step=2480 global_step=2480 loss=119.6, 62.0% complete\n",
            "INFO:tensorflow:local_step=2490 global_step=2490 loss=25.8, 62.2% complete\n",
            "INFO:tensorflow:local_step=2500 global_step=2500 loss=27.8, 62.5% complete\n",
            "INFO:tensorflow:local_step=2510 global_step=2510 loss=27.0, 62.8% complete\n",
            "INFO:tensorflow:local_step=2520 global_step=2520 loss=126.4, 63.0% complete\n",
            "INFO:tensorflow:local_step=2530 global_step=2530 loss=31.7, 63.2% complete\n",
            "INFO:tensorflow:local_step=2540 global_step=2540 loss=29.1, 63.5% complete\n",
            "INFO:tensorflow:local_step=2550 global_step=2550 loss=32.7, 63.8% complete\n",
            "INFO:tensorflow:local_step=2560 global_step=2560 loss=26.2, 64.0% complete\n",
            "INFO:tensorflow:local_step=2570 global_step=2570 loss=120.9, 64.2% complete\n",
            "INFO:tensorflow:local_step=2580 global_step=2580 loss=31.4, 64.5% complete\n",
            "INFO:tensorflow:local_step=2590 global_step=2590 loss=173.0, 64.8% complete\n",
            "INFO:tensorflow:local_step=2600 global_step=2600 loss=27.3, 65.0% complete\n",
            "INFO:tensorflow:local_step=2610 global_step=2610 loss=23.4, 65.2% complete\n",
            "INFO:tensorflow:local_step=2620 global_step=2620 loss=82.6, 65.5% complete\n",
            "INFO:tensorflow:local_step=2630 global_step=2630 loss=25.6, 65.8% complete\n",
            "INFO:tensorflow:local_step=2640 global_step=2640 loss=29.7, 66.0% complete\n",
            "INFO:tensorflow:local_step=2650 global_step=2650 loss=31.6, 66.2% complete\n",
            "INFO:tensorflow:local_step=2660 global_step=2660 loss=33.4, 66.5% complete\n",
            "INFO:tensorflow:local_step=2670 global_step=2670 loss=225.5, 66.8% complete\n",
            "INFO:tensorflow:local_step=2680 global_step=2680 loss=32.9, 67.0% complete\n",
            "INFO:tensorflow:local_step=2690 global_step=2690 loss=157.2, 67.2% complete\n",
            "INFO:tensorflow:local_step=2700 global_step=2700 loss=27.0, 67.5% complete\n",
            "INFO:tensorflow:local_step=2710 global_step=2710 loss=30.1, 67.8% complete\n",
            "INFO:tensorflow:local_step=2720 global_step=2720 loss=191.9, 68.0% complete\n",
            "INFO:tensorflow:local_step=2730 global_step=2730 loss=30.6, 68.2% complete\n",
            "INFO:tensorflow:local_step=2740 global_step=2740 loss=30.1, 68.5% complete\n",
            "INFO:tensorflow:local_step=2750 global_step=2750 loss=30.9, 68.8% complete\n",
            "INFO:tensorflow:local_step=2760 global_step=2760 loss=32.7, 69.0% complete\n",
            "INFO:tensorflow:local_step=2770 global_step=2770 loss=29.4, 69.2% complete\n",
            "INFO:tensorflow:local_step=2780 global_step=2780 loss=31.6, 69.5% complete\n",
            "INFO:tensorflow:local_step=2790 global_step=2790 loss=28.4, 69.8% complete\n",
            "INFO:tensorflow:local_step=2800 global_step=2800 loss=27.1, 70.0% complete\n",
            "INFO:tensorflow:local_step=2810 global_step=2810 loss=26.3, 70.2% complete\n",
            "INFO:tensorflow:local_step=2820 global_step=2820 loss=27.3, 70.5% complete\n",
            "INFO:tensorflow:local_step=2830 global_step=2830 loss=29.1, 70.8% complete\n",
            "INFO:tensorflow:local_step=2840 global_step=2840 loss=26.7, 71.0% complete\n",
            "INFO:tensorflow:local_step=2850 global_step=2850 loss=31.5, 71.2% complete\n",
            "INFO:tensorflow:local_step=2860 global_step=2860 loss=28.6, 71.5% complete\n",
            "INFO:tensorflow:local_step=2870 global_step=2870 loss=25.4, 71.8% complete\n",
            "INFO:tensorflow:local_step=2880 global_step=2880 loss=29.2, 72.0% complete\n",
            "INFO:tensorflow:local_step=2890 global_step=2890 loss=30.8, 72.2% complete\n",
            "INFO:tensorflow:local_step=2900 global_step=2900 loss=29.1, 72.5% complete\n",
            "INFO:tensorflow:local_step=2910 global_step=2910 loss=25.8, 72.8% complete\n",
            "INFO:tensorflow:local_step=2920 global_step=2920 loss=232.8, 73.0% complete\n",
            "INFO:tensorflow:local_step=2930 global_step=2930 loss=28.0, 73.2% complete\n",
            "INFO:tensorflow:local_step=2940 global_step=2940 loss=27.1, 73.5% complete\n",
            "INFO:tensorflow:local_step=2950 global_step=2950 loss=31.2, 73.8% complete\n",
            "INFO:tensorflow:local_step=2960 global_step=2960 loss=30.1, 74.0% complete\n",
            "INFO:tensorflow:local_step=2970 global_step=2970 loss=31.7, 74.2% complete\n",
            "INFO:tensorflow:local_step=2980 global_step=2980 loss=172.3, 74.5% complete\n",
            "INFO:tensorflow:local_step=2990 global_step=2990 loss=31.6, 74.8% complete\n",
            "INFO:tensorflow:local_step=3000 global_step=3000 loss=25.5, 75.0% complete\n",
            "INFO:tensorflow:local_step=3010 global_step=3010 loss=134.3, 75.2% complete\n",
            "INFO:tensorflow:local_step=3020 global_step=3020 loss=29.8, 75.5% complete\n",
            "INFO:tensorflow:local_step=3030 global_step=3030 loss=27.6, 75.8% complete\n",
            "INFO:tensorflow:local_step=3040 global_step=3040 loss=31.8, 76.0% complete\n",
            "INFO:tensorflow:local_step=3050 global_step=3050 loss=31.8, 76.2% complete\n",
            "INFO:tensorflow:local_step=3060 global_step=3060 loss=25.0, 76.5% complete\n",
            "INFO:tensorflow:local_step=3070 global_step=3070 loss=33.2, 76.8% complete\n",
            "INFO:tensorflow:local_step=3080 global_step=3080 loss=31.7, 77.0% complete\n",
            "INFO:tensorflow:local_step=3090 global_step=3090 loss=28.6, 77.2% complete\n",
            "INFO:tensorflow:local_step=3100 global_step=3100 loss=28.3, 77.5% complete\n",
            "INFO:tensorflow:local_step=3110 global_step=3110 loss=26.1, 77.8% complete\n",
            "INFO:tensorflow:local_step=3120 global_step=3120 loss=24.0, 78.0% complete\n",
            "INFO:tensorflow:local_step=3130 global_step=3130 loss=33.5, 78.2% complete\n",
            "INFO:tensorflow:local_step=3140 global_step=3140 loss=32.0, 78.5% complete\n",
            "INFO:tensorflow:local_step=3150 global_step=3150 loss=32.2, 78.8% complete\n",
            "INFO:tensorflow:local_step=3160 global_step=3160 loss=33.6, 79.0% complete\n",
            "INFO:tensorflow:local_step=3170 global_step=3170 loss=26.8, 79.2% complete\n",
            "INFO:tensorflow:local_step=3180 global_step=3180 loss=29.7, 79.5% complete\n",
            "INFO:tensorflow:local_step=3190 global_step=3190 loss=27.4, 79.8% complete\n",
            "INFO:tensorflow:local_step=3200 global_step=3200 loss=23.3, 80.0% complete\n",
            "INFO:tensorflow:local_step=3210 global_step=3210 loss=45.6, 80.2% complete\n",
            "INFO:tensorflow:local_step=3220 global_step=3220 loss=30.8, 80.5% complete\n",
            "INFO:tensorflow:local_step=3230 global_step=3230 loss=29.4, 80.8% complete\n",
            "INFO:tensorflow:local_step=3240 global_step=3240 loss=31.1, 81.0% complete\n",
            "INFO:tensorflow:local_step=3250 global_step=3250 loss=34.2, 81.2% complete\n",
            "INFO:tensorflow:local_step=3260 global_step=3260 loss=30.1, 81.5% complete\n",
            "INFO:tensorflow:local_step=3270 global_step=3270 loss=32.7, 81.8% complete\n",
            "INFO:tensorflow:local_step=3280 global_step=3280 loss=27.3, 82.0% complete\n",
            "INFO:tensorflow:local_step=3290 global_step=3290 loss=29.2, 82.2% complete\n",
            "INFO:tensorflow:local_step=3300 global_step=3300 loss=30.2, 82.5% complete\n",
            "INFO:tensorflow:local_step=3310 global_step=3310 loss=31.1, 82.8% complete\n",
            "INFO:tensorflow:local_step=3320 global_step=3320 loss=29.3, 83.0% complete\n",
            "INFO:tensorflow:local_step=3330 global_step=3330 loss=27.3, 83.2% complete\n",
            "INFO:tensorflow:local_step=3340 global_step=3340 loss=27.2, 83.5% complete\n",
            "INFO:tensorflow:local_step=3350 global_step=3350 loss=32.5, 83.8% complete\n",
            "INFO:tensorflow:local_step=3360 global_step=3360 loss=32.5, 84.0% complete\n",
            "INFO:tensorflow:local_step=3370 global_step=3370 loss=29.1, 84.2% complete\n",
            "INFO:tensorflow:local_step=3380 global_step=3380 loss=27.7, 84.5% complete\n",
            "INFO:tensorflow:local_step=3390 global_step=3390 loss=30.4, 84.8% complete\n",
            "INFO:tensorflow:local_step=3400 global_step=3400 loss=25.6, 85.0% complete\n",
            "INFO:tensorflow:local_step=3410 global_step=3410 loss=30.8, 85.2% complete\n",
            "INFO:tensorflow:local_step=3420 global_step=3420 loss=27.2, 85.5% complete\n",
            "INFO:tensorflow:Recording summary at step 3427.\n",
            "INFO:tensorflow:global_step/sec: 57.1217\n",
            "INFO:tensorflow:local_step=3430 global_step=3430 loss=131.7, 85.8% complete\n",
            "INFO:tensorflow:local_step=3440 global_step=3440 loss=23.8, 86.0% complete\n",
            "INFO:tensorflow:local_step=3450 global_step=3450 loss=32.4, 86.2% complete\n",
            "INFO:tensorflow:local_step=3460 global_step=3460 loss=30.4, 86.5% complete\n",
            "INFO:tensorflow:local_step=3470 global_step=3470 loss=35.2, 86.8% complete\n",
            "INFO:tensorflow:local_step=3480 global_step=3480 loss=30.3, 87.0% complete\n",
            "INFO:tensorflow:local_step=3490 global_step=3490 loss=29.4, 87.2% complete\n",
            "INFO:tensorflow:local_step=3500 global_step=3500 loss=25.3, 87.5% complete\n",
            "INFO:tensorflow:local_step=3510 global_step=3510 loss=23.6, 87.8% complete\n",
            "INFO:tensorflow:local_step=3520 global_step=3520 loss=31.6, 88.0% complete\n",
            "INFO:tensorflow:local_step=3530 global_step=3530 loss=29.2, 88.2% complete\n",
            "INFO:tensorflow:local_step=3540 global_step=3540 loss=31.9, 88.5% complete\n",
            "INFO:tensorflow:local_step=3550 global_step=3550 loss=30.8, 88.8% complete\n",
            "INFO:tensorflow:local_step=3560 global_step=3560 loss=34.3, 89.0% complete\n",
            "INFO:tensorflow:local_step=3570 global_step=3570 loss=29.1, 89.2% complete\n",
            "INFO:tensorflow:local_step=3580 global_step=3580 loss=25.2, 89.5% complete\n",
            "INFO:tensorflow:local_step=3590 global_step=3590 loss=178.6, 89.8% complete\n",
            "INFO:tensorflow:local_step=3600 global_step=3600 loss=32.5, 90.0% complete\n",
            "INFO:tensorflow:local_step=3610 global_step=3610 loss=29.2, 90.2% complete\n",
            "INFO:tensorflow:local_step=3620 global_step=3620 loss=30.5, 90.5% complete\n",
            "INFO:tensorflow:local_step=3630 global_step=3630 loss=30.2, 90.8% complete\n",
            "INFO:tensorflow:local_step=3640 global_step=3640 loss=25.0, 91.0% complete\n",
            "INFO:tensorflow:local_step=3650 global_step=3650 loss=27.7, 91.2% complete\n",
            "INFO:tensorflow:local_step=3660 global_step=3660 loss=33.1, 91.5% complete\n",
            "INFO:tensorflow:local_step=3670 global_step=3670 loss=147.2, 91.8% complete\n",
            "INFO:tensorflow:local_step=3680 global_step=3680 loss=31.6, 92.0% complete\n",
            "INFO:tensorflow:local_step=3690 global_step=3690 loss=29.2, 92.2% complete\n",
            "INFO:tensorflow:local_step=3700 global_step=3700 loss=28.3, 92.5% complete\n",
            "INFO:tensorflow:local_step=3710 global_step=3710 loss=27.1, 92.8% complete\n",
            "INFO:tensorflow:local_step=3720 global_step=3720 loss=29.5, 93.0% complete\n",
            "INFO:tensorflow:local_step=3730 global_step=3730 loss=32.0, 93.2% complete\n",
            "INFO:tensorflow:local_step=3740 global_step=3740 loss=31.3, 93.5% complete\n",
            "INFO:tensorflow:local_step=3750 global_step=3750 loss=31.5, 93.8% complete\n",
            "INFO:tensorflow:local_step=3760 global_step=3760 loss=30.3, 94.0% complete\n",
            "INFO:tensorflow:local_step=3770 global_step=3770 loss=30.8, 94.2% complete\n",
            "INFO:tensorflow:local_step=3780 global_step=3780 loss=27.2, 94.5% complete\n",
            "INFO:tensorflow:local_step=3790 global_step=3790 loss=26.1, 94.8% complete\n",
            "INFO:tensorflow:local_step=3800 global_step=3800 loss=29.2, 95.0% complete\n",
            "INFO:tensorflow:local_step=3810 global_step=3810 loss=25.7, 95.2% complete\n",
            "INFO:tensorflow:local_step=3820 global_step=3820 loss=134.5, 95.5% complete\n",
            "INFO:tensorflow:local_step=3830 global_step=3830 loss=31.7, 95.8% complete\n",
            "INFO:tensorflow:local_step=3840 global_step=3840 loss=24.4, 96.0% complete\n",
            "INFO:tensorflow:local_step=3850 global_step=3850 loss=32.5, 96.2% complete\n",
            "INFO:tensorflow:local_step=3860 global_step=3860 loss=28.3, 96.5% complete\n",
            "INFO:tensorflow:local_step=3870 global_step=3870 loss=28.1, 96.8% complete\n",
            "INFO:tensorflow:local_step=3880 global_step=3880 loss=22.8, 97.0% complete\n",
            "INFO:tensorflow:local_step=3890 global_step=3890 loss=30.1, 97.2% complete\n",
            "INFO:tensorflow:local_step=3900 global_step=3900 loss=26.5, 97.5% complete\n",
            "INFO:tensorflow:local_step=3910 global_step=3910 loss=29.1, 97.8% complete\n",
            "INFO:tensorflow:local_step=3920 global_step=3920 loss=162.6, 98.0% complete\n",
            "INFO:tensorflow:local_step=3930 global_step=3930 loss=30.4, 98.2% complete\n",
            "INFO:tensorflow:local_step=3940 global_step=3940 loss=27.2, 98.5% complete\n",
            "INFO:tensorflow:local_step=3950 global_step=3950 loss=164.0, 98.8% complete\n",
            "INFO:tensorflow:local_step=3960 global_step=3960 loss=27.3, 99.0% complete\n",
            "INFO:tensorflow:local_step=3970 global_step=3970 loss=31.0, 99.2% complete\n",
            "INFO:tensorflow:local_step=3980 global_step=3980 loss=29.1, 99.5% complete\n",
            "INFO:tensorflow:local_step=3990 global_step=3990 loss=25.2, 99.8% complete\n",
            "INFO:tensorflow:local_step=4000 global_step=4000 loss=27.4, 100.0% complete\n",
            "WARNING:tensorflow:Issue encountered when serializing global_step.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Tensor' object has no attribute 'to_proto'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oEDrblTXiU_B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This should take a few minutes, depending on your machine.\n",
        "The result is a list of files in the specified output folder, including:\n",
        " - checkpoints of the model\n",
        " - `tsv` files for the column and row embeddings."
      ]
    },
    {
      "metadata": {
        "id": "_BOhXpZiiU_C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "b1b0463f-baeb-4583-cf61-2e402c1e0fdc"
      },
      "cell_type": "code",
      "source": [
        "%ls {vec_path}"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\n",
            "col_embedding.tsv\n",
            "events.out.tfevents.1539016377.cc4a08418f7e\n",
            "graph.pbtxt\n",
            "model.ckpt-0.data-00000-of-00001\n",
            "model.ckpt-0.index\n",
            "model.ckpt-0.meta\n",
            "model.ckpt-4000.data-00000-of-00001\n",
            "model.ckpt-4000.index\n",
            "model.ckpt-4000.meta\n",
            "row_embedding.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CbZo4fB9WaQ6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One thing missing from the output folder is a file with just the vocabulary, which we'll need later on. We copy this file from the folder with the co-occurrenc matrix."
      ]
    },
    {
      "metadata": {
        "id": "lCOwkAJHWN14",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%cp {coocs_path}/row_vocab.txt {vec_path}vocab.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xI0JCPPpiU_G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Convert `tsv` files to `bin` file\n",
        "The `tsv` files are easy to inspect, but they take too much space and they are slow to load since we need to convert the different values to floats and pack them as vectors. Swivel offers a utility to convert the `tsv` files into a `bin`ary format. At the same time it combines the column and row embeddings into a single space (it simply adds the two vectors for each word in the vocabulary)."
      ]
    },
    {
      "metadata": {
        "id": "qn0FHiYJiU_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "02e50410-1da7-4ba5-ed25-9066935b1e70"
      },
      "cell_type": "code",
      "source": [
        "!python /content/tutorial/scripts/swivel/text2bin.py --vocab={vec_path}vocab.txt --output={vec_path}vecs.bin \\\n",
        "        {vec_path}row_embedding.tsv \\\n",
        "        {vec_path}col_embedding.tsv"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "executing text2bin\n",
            "merging files ['/content/umbc/vec/t_5K/row_embedding.tsv', '/content/umbc/vec/t_5K/col_embedding.tsv'] into output bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MrAWhqbViU_M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This adds the `vocab.txt` and `vecs.bin` to the folder with the vectors:"
      ]
    },
    {
      "metadata": {
        "id": "JX6hOIGqiU_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "62c972cd-4756-4f5e-a0dd-1ab7625d7b97"
      },
      "cell_type": "code",
      "source": [
        "%ls -lah {vec_path}"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 56M\n",
            "drwxr-xr-x 2 root root 4.0K Oct  8 16:36 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "drwxr-xr-x 3 root root 4.0K Oct  8 16:32 \u001b[01;34m..\u001b[0m/\n",
            "-rw-r--r-- 1 root root  199 Oct  8 16:34 checkpoint\n",
            "-rw-r--r-- 1 root root 8.5M Oct  8 16:34 col_embedding.tsv\n",
            "-rw-r--r-- 1 root root 225K Oct  8 16:34 events.out.tfevents.1539016377.cc4a08418f7e\n",
            "-rw-r--r-- 1 root root 291K Oct  8 16:32 graph.pbtxt\n",
            "-rw-r--r-- 1 root root  18M Oct  8 16:32 model.ckpt-0.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root  366 Oct  8 16:32 model.ckpt-0.index\n",
            "-rw-r--r-- 1 root root 116K Oct  8 16:32 model.ckpt-0.meta\n",
            "-rw-r--r-- 1 root root  18M Oct  8 16:34 model.ckpt-4000.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root  366 Oct  8 16:34 model.ckpt-4000.index\n",
            "-rw-r--r-- 1 root root 116K Oct  8 16:34 model.ckpt-4000.meta\n",
            "-rw-r--r-- 1 root root 8.6M Oct  8 16:34 row_embedding.tsv\n",
            "-rw-r--r-- 1 root root 3.0M Oct  8 16:36 vecs.bin\n",
            "-rw-r--r-- 1 root root  43K Oct  8 16:36 vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hY-1lWr3iU_R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Read stored binary embeddings and inspect them\n",
        "\n",
        "Swivel provides the `vecs` library which implements the basic `Vecs` class. It accepts a `vocab_file` and a file for the binary serialization of the vectors (`vecs.bin`)."
      ]
    },
    {
      "metadata": {
        "id": "MAB_mb9ViU_R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tutorial.scripts.swivel import vecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ifovawLuiU_U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "...and we can load existing vectors. We assume you managed to generate the embeddings by following the tutorial up to now. Note that,  due to random initialization of weight during the training step, your results may be different from the ones presented below."
      ]
    },
    {
      "metadata": {
        "id": "y4l1xtJNiU_W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "267c6dce-8039-43f0-9c88-0839592f1747"
      },
      "cell_type": "code",
      "source": [
        "#uncommend the following two lines if you did not manage to train embedding above \n",
        "#!tar -xzf /content/tutorial/datasamples/umbc_swivel_vec_t_5K.tar.gz -C / \n",
        "#vec_path = /content/umbc/vec/t_5K/\n",
        "vectors = vecs.Vecs(vec_path + 'vocab.txt', \n",
        "            vec_path + 'vecs.bin')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Opening vector with expected size 5120 from file /content/umbc/vec/t_5K/vocab.txt\n",
            "vocab size 5120 (unique 5120)\n",
            "read rows\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VjgSQtsPiU_Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have extended the standard implementation of `swivel.vecs.Vecs` to include a method `k_neighbors`. It accepts a string with the word and an optional `k` parameter, that defaults to $10$. It returns a list of python dictionaries with fields:\n",
        "  * `word`: a word in the vocabulary that is near the input word\n",
        "  * `cosim`: the cosine similiarity between the input word and the near word.\n",
        "It's easier to display the results as a `pandas` table:"
      ]
    },
    {
      "metadata": {
        "id": "2iZ9bExziU_e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "8b08f40c-ef73-41ac-9e0a-5586a81586d2"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(vectors.k_neighbors('california'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cosim</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>california</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.422428</td>\n",
              "      <td>santa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.386643</td>\n",
              "      <td>university</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.376657</td>\n",
              "      <td>berkeley</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.371043</td>\n",
              "      <td>barbara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.337771</td>\n",
              "      <td>california,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.336956</td>\n",
              "      <td>southern</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.332485</td>\n",
              "      <td>melvyl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.331769</td>\n",
              "      <td>state</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.301233</td>\n",
              "      <td>recommender</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      cosim         word\n",
              "0  1.000000   california\n",
              "1  0.422428        santa\n",
              "2  0.386643   university\n",
              "3  0.376657     berkeley\n",
              "4  0.371043      barbara\n",
              "5  0.337771  california,\n",
              "6  0.336956     southern\n",
              "7  0.332485       melvyl\n",
              "8  0.331769        state\n",
              "9  0.301233  recommender"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "RfCmWTFsiU_i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "d591beb2-5d45-430c-bc1a-ec1dce7347ba"
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(vectors.k_neighbors('knowledge'))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cosim</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>knowledge</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.395314</td>\n",
              "      <td>organization</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.385889</td>\n",
              "      <td>workers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.355162</td>\n",
              "      <td>warehousing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.336204</td>\n",
              "      <td>skills</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.323475</td>\n",
              "      <td>procedural</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.315733</td>\n",
              "      <td>expertise</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.315241</td>\n",
              "      <td>management</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.300930</td>\n",
              "      <td>exchange</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.298531</td>\n",
              "      <td>exploiting</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      cosim          word\n",
              "0  1.000000     knowledge\n",
              "1  0.395314  organization\n",
              "2  0.385889       workers\n",
              "3  0.355162   warehousing\n",
              "4  0.336204        skills\n",
              "5  0.323475    procedural\n",
              "6  0.315733     expertise\n",
              "7  0.315241    management\n",
              "8  0.300930      exchange\n",
              "9  0.298531    exploiting"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "iwO2zXERiU_o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "a8753613-c66a-48fa-b5aa-d2b30720cb0b"
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(vectors.k_neighbors('semantic'))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cosim</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>semantic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.402217</td>\n",
              "      <td>heterogeneity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.340584</td>\n",
              "      <td>similarities</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.337173</td>\n",
              "      <td>employs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.336691</td>\n",
              "      <td>spaces</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.321805</td>\n",
              "      <td>common</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.321429</td>\n",
              "      <td>expressing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.314453</td>\n",
              "      <td>deciding</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.312328</td>\n",
              "      <td>procedural</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.307621</td>\n",
              "      <td>relationships</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      cosim           word\n",
              "0  1.000000       semantic\n",
              "1  0.402217  heterogeneity\n",
              "2  0.340584   similarities\n",
              "3  0.337173        employs\n",
              "4  0.336691         spaces\n",
              "5  0.321805         common\n",
              "6  0.321429     expressing\n",
              "7  0.314453       deciding\n",
              "8  0.312328     procedural\n",
              "9  0.307621  relationships"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "-Ze0ApZMluvX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "0e2c1cfc-0774-4865-d337-298ec5bf0e67"
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(vectors.k_neighbors('conference'))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cosim</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>conference</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.442867</td>\n",
              "      <td>secretariat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.433434</td>\n",
              "      <td>annual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.398876</td>\n",
              "      <td>workshops</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.380836</td>\n",
              "      <td>2005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.364306</td>\n",
              "      <td>international</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.354203</td>\n",
              "      <td>jcdl</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.333424</td>\n",
              "      <td>fee</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.324881</td>\n",
              "      <td>presentations</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.312933</td>\n",
              "      <td>amia</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      cosim           word\n",
              "0  1.000000     conference\n",
              "1  0.442867    secretariat\n",
              "2  0.433434         annual\n",
              "3  0.398876      workshops\n",
              "4  0.380836           2005\n",
              "5  0.364306  international\n",
              "6  0.354203           jcdl\n",
              "7  0.333424            fee\n",
              "8  0.324881  presentations\n",
              "9  0.312933           amia"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "SzGSaASBfeLU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The cells above should display results similar the the following (for words *california* and *conference*):\n",
        "\n",
        "|\tcosim\t| word | | cosim\t| word |\n",
        "| ---------- | -------- || ---------- | -------- |\n",
        "| 0\t1.000 |\tcalifornia ||\t1.0000\t| conference |\n",
        "| 0.5060 |\tuniversity ||\t0.4320\t| international |\n",
        "| 0.4239 |\tberkeley ||\t0.4063\t| secretariat |\n",
        "| 0.4103 |\tbarbara ||\t0.3857\t| jcdl |\n",
        "|\t0.3941 |\tsanta ||\t0.3798\t| annual |\n",
        "| 0.3899 |\tsouthern ||\t0.3708\t| conferences |\n",
        "| 0.3673 |\tuc ||\t0.3705\t| forum |\n",
        "| 0.3542 |\tjohns ||\t0.3629\t| presentations |\n",
        "| 0.3396 |\tindiana ||\t0.3601\t| workshop |\n",
        "| 0.3388 | melvy ||\t0.3580\t| ... |\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AAqlranRiOE9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compound words\n",
        "\n",
        "Note that the vocabulary only has single words, i.e. compound words are not present:"
      ]
    },
    {
      "metadata": {
        "id": "3Mcdr8dIiU_v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "fcbad8a1-59f9-48c0-a303-6923cf821c4b"
      },
      "cell_type": "code",
      "source": [
        "pd.DataFrame(vectors.k_neighbors('semantic web'))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"semantic web\" is not in vocab, try \"alternative\"\n",
            "semantic web is not in the vocabulary, try e.g. licenses\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "SNc2267PknJY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A common way to work around this issue is to use the average vector of the two individual words (of course this only works if both words are in the vocabulary):"
      ]
    },
    {
      "metadata": {
        "id": "Aj0fNR6Li5mh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "948f6a59-46e4-4295-a27f-a0326b73fad6"
      },
      "cell_type": "code",
      "source": [
        "semantic_vec = vectors.lookup('semantic')\n",
        "web_vec = vectors.lookup('web')\n",
        "semweb_vec = (semantic_vec + web_vec)/2\n",
        "pd.DataFrame(vectors.k_neighbors(semweb_vec))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cosim</th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.554806</td>\n",
              "      <td>web</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.554806</td>\n",
              "      <td>semantic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.294989</td>\n",
              "      <td>employs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.263869</td>\n",
              "      <td>sites</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.258899</td>\n",
              "      <td>crawling</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.250607</td>\n",
              "      <td>crawler</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.239681</td>\n",
              "      <td>browser</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.234558</td>\n",
              "      <td>characterizing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.234176</td>\n",
              "      <td>common</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.222663</td>\n",
              "      <td>remarkable</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      cosim            word\n",
              "0  0.554806             web\n",
              "1  0.554806        semantic\n",
              "2  0.294989         employs\n",
              "3  0.263869           sites\n",
              "4  0.258899        crawling\n",
              "5  0.250607         crawler\n",
              "6  0.239681         browser\n",
              "7  0.234558  characterizing\n",
              "8  0.234176          common\n",
              "9  0.222663      remarkable"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "bJAlDOIZiU_z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we used swivel to generate word embeddings and we explored the resulting embeddings using `k neighbors` exploration. "
      ]
    },
    {
      "metadata": {
        "id": "tvn6g8WCLieL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Optional Excercise\n",
        "\n",
        "## Create word-embeddings for texts from Project Gutenburg\n",
        "\n",
        "### Download and pre-process the corpus\n",
        "\n",
        "You can try generating new embeddings using a small `gutenberg` corpus, that is provided as part of the NLTK library. It consists of a few public-domain works published as part of the Project Gutenberg.\n",
        "\n",
        "First, we download the dataset into out environment:"
      ]
    },
    {
      "metadata": {
        "id": "3CQi8I19iU-x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "f7325e22-65d9-44d0-ff9e-ee85ece593c0"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "%ls '/root/nltk_data/corpora/gutenberg/'"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "austen-emma.txt          carroll-alice.txt        README\n",
            "austen-persuasion.txt    chesterton-ball.txt      shakespeare-caesar.txt\n",
            "austen-sense.txt         chesterton-brown.txt     shakespeare-hamlet.txt\n",
            "bible-kjv.txt            chesterton-thursday.txt  shakespeare-macbeth.txt\n",
            "blake-poems.txt          edgeworth-parents.txt    whitman-leaves.txt\n",
            "bryant-stories.txt       melville-moby_dick.txt\n",
            "burgess-busterbrown.txt  milton-paradise.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hrh3N-bq72We",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, the corpus consists of various books, one per file. Most word2vec implementations require you to pass a corpus as a single text file. We can issue a few commands to do this by concatenating all the `txt` files in the folder into a single `all.txt` file, which we will use later on.\n",
        "\n",
        "A couple of the files are encoded using iso-8859-1 or binary encodings, which will cause trouble later on, so we rename them to avoid including them into our corpus."
      ]
    },
    {
      "metadata": {
        "id": "6UxGMz7ONMnf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "8ec2d8b7-ec69-4884-9ea8-4ade5be482c0"
      },
      "cell_type": "code",
      "source": [
        "%cd /root/nltk_data/corpora/gutenberg/\n",
        "# avoid including books with incorrect encoding\n",
        "!mv chesterton-ball.txt chesterton-ball.badenc-txt\n",
        "!mv milton-paradise.txt milton-paradise.badenc-txt\n",
        "!mv shakespeare-caesar.txt shakespeare-caesar.badenc-txt\n",
        "# now concatenate all other files into 'all.txt'\n",
        "!cat *.txt >> all.txt\n",
        "# print result\n",
        "%ls -lah '/root/nltk_data/corpora/gutenberg/all.txt'\n",
        "# go back to standard folder \n",
        "%cd /content/"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/nltk_data/corpora/gutenberg\n",
            "-rw-r--r-- 1 root root 11M Oct  8 16:39 /root/nltk_data/corpora/gutenberg/all.txt\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lf2qQSE98VuR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The full dataset is about 11MB."
      ]
    },
    {
      "metadata": {
        "id": "XfD_c2yYOMIt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Learn embeddings\n",
        "\n",
        "Run the steps described above to generate embeddings for the gutenberg dataset."
      ]
    },
    {
      "metadata": {
        "id": "wbAI-zc0OXL9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sd4bvnH6OZic",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inspect embeddings\n",
        "Use methods similar to the ones shown above to get a feeling for whether the generated embeddings have captured interesting relations between words."
      ]
    },
    {
      "metadata": {
        "id": "ZksSgO2GOcnP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}